## 脑裂

脑裂：指在主从集群中，同时有两个主节点，它们都能接收写请求，客户端不知道应该往哪个主节点写入数据，结果就是不同的客户端会往不同的主节点上写入数据，进一步导致数据丢失

### 产生原因

1. 主库的数据还没有同步到从库，结果主库发生了故障，等从库升级为主库后，未同步的数据就丢失了

排查
  - 通过比对主从库上的复制进度差值来进行判断，也就是计算 master_repl_offset 和 slave_repl_offset 的差值判断数据丢失是否由同步未完成导致
  - 客户端操作日志
  - 原主库由于压力大导致出现无法响应心跳，假故障，哨兵选举出一个新主库
    - 和主库部署在同一台服务器上的其他程序临时占用了大量资源（例如 CPU 资源），导致主库资源使用受限，短时间内无法响应心跳。其它程序不再使用资源时，主库又恢复正常
    - 主库自身遇到了阻塞的情况，例如，处理 bigkey 或是发生内存 swap，短时间内无法响应心跳，等主库阻塞解除后，又恢复正常的请求处理

### 导致数据丢失

主从切换后，从库一旦升级为新主库，哨兵就会让原主库执行 slave of 命令，和新主库重新进行全量同步。而在全量同步执行的最后阶段，原主库需要清空本地的数据，加载新主库发送的 RDB 文件，这样一来，原主库在主从切换期间保存的新写数据就丢失了

### 解决

通过配置项限制主库的请求处理
  -min-slaves-to-write：主库能进行数据同步的最少从库数量（K/2+1）
  -min-slaves-max-lag：主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（10～20s）

合理设置配置项，即使原主库出现假故障无法响应哨兵心跳，也不能和从库进行同步，这样原主库就会被限制接收客户端请求进行读写，新主库上线后原主库降为从库，没有新写的数据，不会出现数据丢失
