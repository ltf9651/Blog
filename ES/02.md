## 倒排索引与分词

正排索引

id | 文档
---------|----------|---------
 1 | good morning
 2 | good night

倒排索引

单词 | 文档 ID 列表
---------|----------|---------
 good | 1,2
 morning | 2
 night | 2

查询过程
  1. 通过到倒排索引获取对应的文档 ID 列表
  1. 通过正排索引查询文档 ID 对应的内容
  1. 返回用户最终结果

### 倒排索引

倒排索引是搜索引擎的核心
  - 单词词典（Term Dictionary）
    - 记录所有文档的单词
    - 记录单词到倒排列表的关联信息
  - 倒排列表（Posting List）
    - 记录了单词对应的文档集合，由倒排索引项（Posting）组成
    - 倒排索引项
      - 文档 Id，用于获取原始信息
      - 单词频率（Term Frequency），单词在文档中出现的次数
      - 位置（Position)，单词在文档中的分词位置（多个），用于做词语搜索（Phrase Query）
      - 偏移（Offset），单词在文档的开始和结束位置，用于做高亮显示

搜索‘good‘的倒排列表
DocId | TF | Position | Offset
---------|----------|---------|---------
 1 | 1 | 0 | <0,4>
 2 | 1 | 0 | <0,4>

es 存储的是一个 json 格式的文档，json 包含多个字段，每个字段会有自己的倒排索引

### 分词

分词：将文本转换成一些列单词（Term or token）的过程，也叫文本分析（Analysis）

`elasticsearch 是很流行的搜索引擎` -> `elasticsearch`，`流行`，`搜索引擎`

分词器（Analyzer）
  - Character Filters 针对原始文本进行处理，比如取出 html 特殊标记符
  - Tokenizer 将原始文本按照一定规则划分为单词
  - Token Filters 针对 tokenizer 处理的单词就行再加工，比如转小写、删除、新增等

Analyzer API

```
POST _analyze
{
  "analyzer":"standard",
  "tokenizer":"standard",
  "filters":"["lowercase"],
  "text":"hello world"
}
```

es 预定义分词器
  - Standard 按词切分，支持多语言，小写处理
  - Simple 按照非字母切分，小写处理
  - Whitespace 按空格切分
  - Stop 语气助词切分 (the，an, 的，这）
  - Keyword 不分词，直接将输入文本作为一个单词输出
  - Pattern 通过正则表达式自定义分割符，默认非字符`\W+`，小写处理
  - Language 各国多语言分词器

### 中文分词

- 英文单词之间以空格作为分界符，汉语没有
- 容易产生歧义
  - 羽毛球拍 / 卖 / 完了
  - 羽毛球 / 拍卖 / 完了
- 常见分词器
  - IK
  - jieba
  - Hanlp
  - THULAC
