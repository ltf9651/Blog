## 分布式

es 支持集群模式
  - 可增大系统容量，内存、磁盘等
  - 提高可用性

es 集群由多个 es 实例组成
  - 不同集群通过集群名（cluster.name）来区分
  - 每个 es 实例实质上是一个 JVM 进程，可通过 node.name 修改名字

管理工具：cerebro

### 构建集群

`bin/elasticsearch -Ecluster.name=my_cluster -Enode.name=node1 -Ehttp.port=5200 -d`

Cluster State
  - 记录节点集群的综合信息
  - 节点信息：名称、连接地址等
  - 索引信息：索引名、配置等

Master Node
  - 可以修改 cluster state 的节点，一个集群只能有一个
  - cluster state 存储在每个节点上，master 负责维护最新版本并同步给其他节点
  - master 节点通过集群中所有节点选举产生，可被选举的节点称为 master-eligible

coordinating node
  - 处理请求的节点，所有节点的默认角色
  - 路由请求到正确的节点处理，比如创建索引的请求到 master

data node
  - 存储数据的节点
  - 默认节点类型都是 data

### 可用性

- 服务可用性
  - 多个节点的情况下允许其中某些节点停止服务
- 数据可用性
  - 引入副本（Replication）
  - 每个节点都有完整数据

### 增大系统容量

引入分片（Shard）将数据分布于所有节点
  - es 支持 PB 级数据的基石
  - 分片存储部分数据，可以分布于任意节点
  - 分片数在创建索引时指定，后续不可更改，默认 5 个
  - 分片有主副之分，实现数据高可用
  - 副本分片的数据由主分片同步，副分片数可动态调整

```
PUT test_index
{
    "setting":{
        "number_of_shards":3,
        "number_of_replicas":1
    }
}
// 3个主分片，1个副本，总共6个分片
```

### Cluster Health

`GET _cluster/health`
  - green 健康
  - yellow 主分片正常分配，副分片未正常分配
  - red 有主分片未分配

### 故障转移

当 master 宕机，其他 node 会选举一个新 node，主分片下线后集群状态变成 red

新的 master 发现主分片未分配，将其他分片提升为主分片，此时主分片正常分配，集群状态为 yellow

为旧 master 的分片生成新的副本，集群状态 green

### 文档分布式存储

Document 通过映射算法最终存储在分片上

映射算法
  - 使文档均匀分布在所有分片，以充分利用资源
  - 随机、轮询算法等需要维护文档到分片的映射关系，成本高
  - 根据文档值实时计算对应的分片
  - shard = hash(routing) % number_of_primary_shards
  - hash 算法保证数据均匀的分散在分片中
  - routing：文档 id，也可自行指定
  - number_of_primary_shards：主分片数，这也是分片数（number_of_shards）一旦确定便不能修改的原因

创建
  - Client 向某个 node3 发起创建文档的请求
  - 该 node3 通过计算得出存储位置 Shard 1，查询 Cluster State 后确认 Shard 1 在 node2 后将请求转发至 node2
  - node2 接收并执行创建请求，将请求发送到副本分片
  - 副本分片接收并执行创建请求，向主分片发送成功通知
  - 主分片接收副本分片的结果，通知 node3 创建成功，node3 通知 Client

读取
  - Client 向某个 node3 发起读取文档的请求
  - node3 通过计算该文档在 Shard 1，查询 Cluster State 获取 Shard 1 的主副分片列表
  - 以轮询的方式获取一个分片，转发读取文档的请求到相应 node1
  - node1 处理后转发给 node3，node3 返回结果给 Client

批量创建（bulk）
  - Client 向某个 node3 发起批量创建文档的请求
  - 该 node3 通过计算所有文档对应的 shard，然后按照主 shard 分配对应执行的操作，同时发送请求到涉及的主 shard
  - 主 shard 接收并执行请求后，将请求同步到对应的副本 shard
  - 副本 shard 将执行结果返回主 shard，主 shard 返回 node3，node3 返回 Client

批量读取（mget）
  - Client 向某个 node3 发起批量读取文档的请求
  - node3 通过计算得出所有文档对应的 shard，以轮询的机制获取所有参与的 shard，按照 shard 构建 mget 请求，同时发送请求到涉及的 shard
  - shard 返回 node3，node3 返回 Client

### Shard

倒排索引一旦生产即不可更改
  - 不用考虑并发写文件的问题，杜绝锁带来的性能问题
  - 文件不再更改，可以充分利用缓存（不失效）
  - 利于生成缓存，缓存无需维护变更性
  - 利于对文件进行压缩，节省空间
  - 写入新文档时必须重新构建倒排索引文建，然后替换老文档，实时性差
  - 新文档写入时直接生成新的倒排索引文件，查询的时候同时查询所有倒排文件，汇总结果，提高实时性
  - 可通过 refresh 进一步提升实时性 (es 默认 1 秒执行一次）

translog
  - es 启动时会检查 translog，从中恢复数据
  - 写入文档到 buffer 时同时将操作写入 translog
  - translog 会实时 fsync 写入磁盘

flush
  - 将内存的数据写入磁盘
  - 将 translog 写入磁盘
  - 执行 refresh
  - 删除旧 translog
  - 执行时机
    - 间隔时间 index.translog.flush_threshold_period 默认 30 分钟
    - translog 占满时，translog 大小通过`index.translog.flush_threshold_size`控制

refresh 执行时机
  - index.setting.refresh_interval：默认 1 秒
  - index.buffer 缓冲队列占满时，index.buffer 的大小通过 `indices.memory.index_buffer_size`控制
  - flush

.del 文件记录所有已删除的文档，查询结果返回前会过滤。del 文件中的所有文档

更新文档其实是先删除 document 再新建

### 脑裂 Split-brain

node1 是 master node , 掉线后选举出新 node2，node1 和 node2 都会更新自己的 cluster state，网络恢复后无法选择正确的 master

解决：选举时 master-eligible 节点数必须大于等于 quorum 才可以进行选举
  - quorum = master-eligible / 2 +1
  - 设定 `discovery.zen.minimum_master_nodes`为 quorum

掉线后 node1 无法成为 master，网络恢复后加入新 master 的集群
